// Code generated by Speakeasy (https://speakeasyapi.dev). DO NOT EDIT.

package provider

import (
	"context"
	"fmt"
	"github.com/aballiet/terraform-provider-airbyte/internal/sdk"
	"github.com/aballiet/terraform-provider-airbyte/internal/sdk/pkg/models/shared"

	"github.com/hashicorp/terraform-plugin-framework/datasource"
	"github.com/hashicorp/terraform-plugin-framework/datasource/schema"
	"github.com/hashicorp/terraform-plugin-framework/types"
	"github.com/hashicorp/terraform-plugin-framework/types/basetypes"
)

// Ensure provider defined types fully satisfy framework interfaces.
var _ datasource.DataSource = &DestinationDefinitionDataSource{}
var _ datasource.DataSourceWithConfigure = &DestinationDefinitionDataSource{}

func NewDestinationDefinitionDataSource() datasource.DataSource {
	return &DestinationDefinitionDataSource{}
}

// DestinationDefinitionDataSource is the data source implementation.
type DestinationDefinitionDataSource struct {
	client *sdk.SDK
}

// DestinationDefinitionDataSourceModel describes the data model.
type DestinationDefinitionDataSourceModel struct {
	Custom                  types.Bool                               `tfsdk:"custom"`
	DestinationDefinitionID types.String                             `tfsdk:"destination_definition_id"`
	DockerImageTag          types.String                             `tfsdk:"docker_image_tag"`
	DockerRepository        types.String                             `tfsdk:"docker_repository"`
	DocumentationURL        types.String                             `tfsdk:"documentation_url"`
	Icon                    types.String                             `tfsdk:"icon"`
	Name                    types.String                             `tfsdk:"name"`
	NormalizationConfig     NormalizationDestinationDefinitionConfig `tfsdk:"normalization_config"`
	ProtocolVersion         types.String                             `tfsdk:"protocol_version"`
	ReleaseDate             types.String                             `tfsdk:"release_date"`
	ReleaseStage            types.String                             `tfsdk:"release_stage"`
	ResourceRequirements    *ActorDefinitionResourceRequirements     `tfsdk:"resource_requirements"`
	SupportLevel            types.String                             `tfsdk:"support_level"`
	SupportsDbt             types.Bool                               `tfsdk:"supports_dbt"`
}

// Metadata returns the data source type name.
func (r *DestinationDefinitionDataSource) Metadata(ctx context.Context, req datasource.MetadataRequest, resp *datasource.MetadataResponse) {
	resp.TypeName = req.ProviderTypeName + "_destination_definition"
}

// Schema defines the schema for the data source.
func (r *DestinationDefinitionDataSource) Schema(ctx context.Context, req datasource.SchemaRequest, resp *datasource.SchemaResponse) {
	resp.Schema = schema.Schema{
		MarkdownDescription: "DestinationDefinition DataSource",

		Attributes: map[string]schema.Attribute{
			"custom": schema.BoolAttribute{
				Computed: true,
				MarkdownDescription: `Default: false` + "\n" +
					`Whether the connector is custom or not`,
			},
			"destination_definition_id": schema.StringAttribute{
				Required: true,
			},
			"docker_image_tag": schema.StringAttribute{
				Computed: true,
			},
			"docker_repository": schema.StringAttribute{
				Computed: true,
			},
			"documentation_url": schema.StringAttribute{
				Computed: true,
			},
			"icon": schema.StringAttribute{
				Computed: true,
			},
			"name": schema.StringAttribute{
				Computed: true,
			},
			"normalization_config": schema.SingleNestedAttribute{
				Computed: true,
				Attributes: map[string]schema.Attribute{
					"supported": schema.BoolAttribute{
						Computed: true,
						MarkdownDescription: `Default: false` + "\n" +
							`whether the destination definition supports normalization.`,
					},
					"normalization_repository": schema.StringAttribute{
						Computed:    true,
						Description: `a field indicating the name of the repository to be used for normalization. If the value of the flag is NULL - normalization is not used.`,
					},
					"normalization_tag": schema.StringAttribute{
						Computed:    true,
						Description: `a field indicating the tag of the docker repository to be used for normalization.`,
					},
					"normalization_integration_type": schema.StringAttribute{
						Computed:    true,
						Description: `a field indicating the type of integration dialect to use for normalization.`,
					},
				},
				Description: `describes a normalization config for destination definition version`,
			},
			"protocol_version": schema.StringAttribute{
				Computed:    true,
				Description: `The Airbyte Protocol version supported by the connector`,
			},
			"release_date": schema.StringAttribute{
				Computed:    true,
				Description: `The date when this connector was first released, in yyyy-mm-dd format.`,
			},
			"release_stage": schema.StringAttribute{
				Computed:    true,
				Description: `must be one of ["alpha", "beta", "generally_available", "custom"]`,
			},
			"resource_requirements": schema.SingleNestedAttribute{
				Computed: true,
				Attributes: map[string]schema.Attribute{
					"default": schema.SingleNestedAttribute{
						Computed: true,
						Attributes: map[string]schema.Attribute{
							"cpu_request": schema.StringAttribute{
								Computed: true,
							},
							"cpu_limit": schema.StringAttribute{
								Computed: true,
							},
							"memory_request": schema.StringAttribute{
								Computed: true,
							},
							"memory_limit": schema.StringAttribute{
								Computed: true,
							},
						},
						Description: `optional resource requirements to run workers (blank for unbounded allocations)`,
					},
					"job_specific": schema.ListNestedAttribute{
						Computed: true,
						NestedObject: schema.NestedAttributeObject{
							Attributes: map[string]schema.Attribute{
								"job_type": schema.StringAttribute{
									Computed: true,
									MarkdownDescription: `must be one of ["get_spec", "check_connection", "discover_schema", "sync", "reset_connection", "connection_updater", "replicate"]` + "\n" +
										`enum that describes the different types of jobs that the platform runs.`,
								},
								"resource_requirements": schema.SingleNestedAttribute{
									Computed: true,
									Attributes: map[string]schema.Attribute{
										"cpu_request": schema.StringAttribute{
											Computed: true,
										},
										"cpu_limit": schema.StringAttribute{
											Computed: true,
										},
										"memory_request": schema.StringAttribute{
											Computed: true,
										},
										"memory_limit": schema.StringAttribute{
											Computed: true,
										},
									},
									Description: `optional resource requirements to run workers (blank for unbounded allocations)`,
								},
							},
						},
					},
				},
				Description: `actor definition specific resource requirements. if default is set, these are the requirements that should be set for ALL jobs run for this actor definition. it is overriden by the job type specific configurations. if not set, the platform will use defaults. these values will be overriden by configuration at the connection level.`,
			},
			"support_level": schema.StringAttribute{
				Computed:    true,
				Description: `must be one of ["community", "certified", "none"]`,
			},
			"supports_dbt": schema.BoolAttribute{
				Computed:    true,
				Description: `an optional flag indicating whether DBT is used in the normalization. If the flag value is NULL - DBT is not used.`,
			},
		},
	}
}

func (r *DestinationDefinitionDataSource) Configure(ctx context.Context, req datasource.ConfigureRequest, resp *datasource.ConfigureResponse) {
	// Prevent panic if the provider has not been configured.
	if req.ProviderData == nil {
		return
	}

	client, ok := req.ProviderData.(*sdk.SDK)

	if !ok {
		resp.Diagnostics.AddError(
			"Unexpected DataSource Configure Type",
			fmt.Sprintf("Expected *sdk.SDK, got: %T. Please report this issue to the provider developers.", req.ProviderData),
		)

		return
	}

	r.client = client
}

func (r *DestinationDefinitionDataSource) Read(ctx context.Context, req datasource.ReadRequest, resp *datasource.ReadResponse) {
	var data *DestinationDefinitionDataSourceModel
	var item types.Object

	resp.Diagnostics.Append(req.Config.Get(ctx, &item)...)
	if resp.Diagnostics.HasError() {
		return
	}

	resp.Diagnostics.Append(item.As(ctx, &data, basetypes.ObjectAsOptions{
		UnhandledNullAsEmpty:    true,
		UnhandledUnknownAsEmpty: true,
	})...)

	if resp.Diagnostics.HasError() {
		return
	}

	destinationDefinitionID := data.DestinationDefinitionID.ValueString()
	request := shared.DestinationDefinitionIDRequestBody{
		DestinationDefinitionID: destinationDefinitionID,
	}
	res, err := r.client.DestinationDefinition.GetDestinationDefinition(ctx, request)
	if err != nil {
		resp.Diagnostics.AddError("failure to invoke API", err.Error())
		if res != nil && res.RawResponse != nil {
			resp.Diagnostics.AddError("unexpected http request/response", debugResponse(res.RawResponse))
		}
		return
	}
	if res == nil {
		resp.Diagnostics.AddError("unexpected response from API", fmt.Sprintf("%v", res))
		return
	}
	if res.StatusCode != 200 {
		resp.Diagnostics.AddError(fmt.Sprintf("unexpected response from API. Got an unexpected response code %v", res.StatusCode), debugResponse(res.RawResponse))
		return
	}
	if res.DestinationDefinitionRead == nil {
		resp.Diagnostics.AddError("unexpected response from API. No response body", debugResponse(res.RawResponse))
		return
	}
	data.RefreshFromGetResponse(res.DestinationDefinitionRead)

	// Save updated data into Terraform state
	resp.Diagnostics.Append(resp.State.Set(ctx, &data)...)
}
